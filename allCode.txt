#core\conversation\session.py
from core.llm.contract.message import Message

class ConversationSession:
    def __init__(self, system_prompt: str | None = None):
        self.messages: list[Message] = []

        if system_prompt:
            self.messages.append({
                "role": "system",
                "content": system_prompt
            })

    def user(self, content: str) -> None:
        self.messages.append({
            "role": "user",
            "content": content
        })

    def assistant(self, content: str) -> None:
        self.messages.append({
            "role": "assistant",
            "content": content
        })

    def get_messages(self) -> list[Message]:
        return self.messages


# core/llm/contract/function.py
from dataclasses import dataclass
from typing import Any, Dict

@dataclass
class FunctionSpec:
    name: str
    description: str
    parameters: Dict[str, Any]


# core/llm/contract/message.py
from typing import Literal, TypedDict

Role = Literal["system", "user", "assistant", "tool"]

class Message(TypedDict):
    role: Role
    content: str



# core/llm/contract/provider.py
from abc import ABC, abstractmethod
from .message import Message
from .result import LLMResult

class BaseLLMProvider(ABC):

    @abstractmethod
    def generate(self, messages: list[Message]) -> LLMResult:
        pass


# core/llm/contract/result.py
from dataclasses import dataclass
from typing import Union
    
class BaseResult:
    """Marker base class for all LLM results."""
    pass

@dataclass
class TextResult(BaseResult):
    content: str

@dataclass
class FunctionCallResult(BaseResult):
    name: str
    arguments: dict

LLMResult = Union[TextResult, FunctionCallResult]


# core/llm/providers/llama.py
from llama_cpp import Llama
from core.llm.contract.provider import BaseLLMProvider
from core.llm.contract.message import Message
from core.llm.contract.result import TextResult

class LlamaCppProvider(BaseLLMProvider):

    def __init__(
        self,
        model_path: str,
        n_ctx=4096,
        n_threads=8,
        n_batch=512,
    ):
        self.llm = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_threads=n_threads,
            n_batch=n_batch,
        )


    def generate(self, messages: list[Message]) -> TextResult:
        prompt = self._to_prompt(messages)

        output = self.llm(
            prompt,
            max_tokens=512,
            stop=["</s>"],
        )

        text = output["choices"][0]["text"]
        return TextResult(content=text)

    def _to_prompt(self, messages: list[Message]) -> str:
        lines = []
        for m in messages:
            lines.append(f"{m['role']}: {m['content']}")
        lines.append("assistant:")
        return "\n".join(lines)


# core\llm\engine.py
from core.llm.contract.result import BaseResult
from core.llm.contract.provider import BaseLLMProvider

class LLMEngine:
    def __init__(self, provider:BaseLLMProvider):
        self.provider = provider

    def chat(self, session) -> BaseResult:
        result = self.provider.generate(session.get_messages())

        # 基礎對話階段，只接受 TextResult
        if isinstance(result, BaseResult):
            session.assistant(result.content)
            return result

        raise RuntimeError("Unexpected LLMResult type in base chat mode")


# core/llm/factory.py
from core.llm.providers.llama import LlamaCppProvider
from core.llm.providers.openai import OpenAIProvider

def create_provider(profile):
    if profile.type == "llama_cpp":
        return LlamaCppProvider(
            model_path=profile.model_path,
            n_ctx=profile.n_ctx,
            n_threads=profile.n_threads,
            n_batch=profile.n_batch,
        )

    if profile.type == "openai":
        return OpenAIProvider(
            api_key=profile.api_key,
            model_name=profile.model_name,
        )

    raise ValueError(f"Unknown provider type: {profile.type}")
